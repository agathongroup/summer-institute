---
title: "Automated Text Analysis"
author: "Ahmet Kurnaz"
date: "16 06 2020"
output:
  html_document:
    toc: true
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(data.table)
library(quanteda)
library(ggplot2)
library(quanteda.textmodels)
library(caret)
library(stm)
library(text2vec)
library(dplyr)
library(tidytext)
```

## Automated Text Analysis

In this tutotial I will exaplain some of the most important skills which a data scientist should have. 

- First and foremost important step towards a successfull data analysis is that you should have clean data. To clean a messy textual data should be able to use regular expressions at least basic level.  

- Second you should be able to convert textual data to a matrix. In general, these textual matrices are called document feature matrix or document term matrix. 

- Third, you can use text data to predict outcomes by using machine learning.


### Regular Expressions

If you are new to regular expressions you shouldn't be intimidated by them. They may seem like giberish but once you understand the basic structure and how they work, writing codes for cleaning messy textual data will become like writing poems. Using regular expressions are basically the practice of pattern matching . You could combine them with several functions. The most prominent ones are grep, grepl and gsub. Many others have been built based on these foundational functions. 
#### Basic Pattern Matching

Basic matcing is a straightforward operation. You can just write a pattern that you want to match. For example, look the sentences below and see how a regex works

```{r echo=TRUE}
basic_list <- c('This sentence cotains the word regex',
                "This setence doesn't contain that word and isn't short",
                "This sentence is very short")

grepl(pattern = 'regex',x = basic_list)

grepl(pattern = 'sentence',x = basic_list)

grepl(pattern = 'contain',x = basic_list)



```

Now your turn could you write a regular expression to find which sentences contains the word `short`

#### Bracets []

Think about a case that you should convert the values below to "apples". It is really easy when you have only 4 entries. However, think about you have 4 million entries. How could you overcome this problem? There could be several solutions, using regular expressions is the first one comes to my mind. Lets see one way of doing this with regex. 

```{r echo=TRUE}
apple_list <- c('Apples','appleas','app1es','AplEs')

gsub('[Aa][pP].*[eE][sS]','apples',apple_list)

```

We are using character sets to match a pattern. `[Aa]` means either an uppercase letter `A` or a lowercase `a`. `.` means any character and `*` means **zero or more**. 

#### Quantifiers

`sicss*` -> Match the string which has 'sics' followed by zero or more 's'

`sicss+` -> Matches the string which has 'sics' followed by one or more 's'

`sicss?` -> Matches the string which has 'sicss' or 'sics'. The character 's' is optional.

`sics{2}` -> Matches the string which has 'sic' followed by exactly 2 's'

`sicss{2,}` -> Matches the string which has 'sic' followed by 2 or more 's'

`sicss{0,2}` -> Matches the string which has 'sic' followed by 0 to 2 's'

`(sicss)*` -> Matches the string which has zero or more 'sicss'

`(sics)+s` -> Matches the string which has one or more 'sics' and at end 's'

Now your turn, write regex to experiment all of the codes described above. 


```{r echo=TRUE}
quantifier_sententes <- c('Sicss is fun',
                          'sicssss is fun',
                          'What is sic?',
                          'sics istanbul rocks',
                          'sicsS is not sicSsssS')

```

#### Grouping

Let's find all URL's in the text!

```{r echo=TRUE}

group_sentences <- c('www.ahmetkurnaz001.com ',
                     'www.sicss. co memories',
                     'Www Akin Unver commercials',
                     'www.google.com',
                     'www.faceboo .com')

grepl(pattern = 'www\\.([A-z0-9]+)\\.com',x = group_sentences)

```

Could you write a regex to find all URL's below?

```{r echo=FALSE}
urls <- c('www.ahmetkurnaz.net',
          'www.mahmuttuncer.org',
          'www.youtube.com',
          'www.oii.ac.uk')

```


#### Anchors ^ and $

^ matches how a line/pattern starting

$ matches how a line/pattern ends

Let's find sentence that starts with a string and ends with a number

```{r echo=TRUE}

anchor_sentences <- c('Wells, A. (2009). Guilford Press.  com',
                      "Balakrishnan, R. (2006, March 25-26).148",
                      "1997 http://www.mimsonline.com.au",
                      "Munro, C. (1999). http://about.murdoch.edu.au/synergy/0303/grief.com",
                      "Ahmet Kurnaz http://www.ninarva.com")

grepl(pattern = '^[A-z].*[0-9]$',x = anchor_sentences)

```

Could you find the sentence that starts with a string and ends with a URL!

Could you find a sentence that starts with a string, ends with a URL and contains at least one numeric character? 


### Text as Data

Once you have a clean dataset you should be thinking about how to convert it to tabular format. There are many methods and also many pitfalls. You should read [this paper](https://www.cambridge.org/core/journals/political-analysis/article/text-as-data-the-promise-and-pitfalls-of-automatic-content-analysis-methods-for-political-texts/F7AAC8B2909441603FEB25C156448F20) as soon as possible to understand the basics of automated text analysis.

To look behind the scene there are several R packages but quanteda is the rising star. At this point I will try to explain some basic concepts. Corpus, tokens and document-feature matrix are the main concepts.




```{r echo=TRUE}
lang_data <- data.table(text=c('Hello world! I am learning R which is a statistical language and I want to use it!',
                               "Hello, I don't know R statistical language",
                               "I know and use R. However, there is lot to learn!",
                               "I prefer to use Python, a scripting language 1",
                               "Python is a scripting language, 1 2 3"),
                        group=c("R","R","R","P","P"))
```
#### Corpus

**Corpus:** A corpus is a data.frame which contains textual data and additional meta data. It is document level data structure.

Let's create a basic corpus with lang_data.

```{r echo=TRUE}
lang_corp <- corpus(lang_data)

################ docvars
print(lang_corp)
docvars(lang_corp)

lang_corp$group

lang_corp$category <- 'Programming Languages'

docvars(lang_corp)
############## docnames

docnames(lang_corp) <- 1:nrow(lang_data)
print(lang_corp)

docnames(lang_corp)


############ subset corpus

corpus_subset(lang_corp,group=='R')


############# extraction

corpus_segment(x = lang_corp,pattern = "I[a-z\\s\\']*R$",valuetype = 'regex',extract_pattern = F)

```

Now your turn. 
- Please create a corpus and set names to ids. For example for the first three sentence you can use `F_1,F_2` and `F_2` and for the rest `W_1,W_2`. 
- As you can see fourth sentence group is wrong, could you correct it
- Create a new docvar with the name isCorrected and give logical values T or false based on the opeartions
- Could you segment the corpus based on the love quotations such as `'All you need is love'`,  `'love well'`, `'I love you'`

```{r echo=TRUE}

example_data <- data.table(text=c("'All you need is love.' But a little chocolate now and then doesn't hurt.",
                                  "One cannot think well, 'love well', sleep well, if one has not dined well.",
                                  "'I  love you' like a fat kid loves cake!",
                                  "You can’t build a reputation on what you’re going to do.",
                                  "Do or do not. There is no try."),
                           group=c('F','F','F','F','W'))

```

#### Tokens

**Tokens:** Stores tokens in a list of vectors where the sequence of the words are preserved. You can work with ngrams and compound string structures at this level. In other words, you can do positional analysis.

You can create a tokens object by usıng either a character string or your corpues object. However, when you use a corpus object you preserve document names and vars. 

```{r echo=TRUE}

#basics of tokens
lang_tokens.1 <- tokens(lang_data[,text])

lang_tokens.2 <-tokens(lang_corp) 


lang_tokens.1

lang_tokens.2


#do not remove punctuation
tokens(lang_corp,remove_punct = F)

#do not remove numbers
tokens(lang_corp,remove_numbers = F)

#keyword in context

kwic(lang_tokens.2,pattern = 'use')

kwic(lang_tokens.2,pattern = c('R','Python'),window = 3)


#remove tokens

tokens_select(lang_tokens.2,pattern = stopwords(language = 'en'),selection = 'remove')

tokens_select(lang_tokens.2,pattern = c('R','Python'),selection = 'keep',padding = T)


# compound tokens
kwic(lang_tokens.2,pattern = phrase('programming language'))

tokens_compound(lang_tokens.2,pattern = phrase('programming language'))




```


Now your turn. Use example_corpus to create tokens object


```{r echo=TRUE}



```

#### Document-feature Matrix (DFM)

**DFM:** A document feature matrix is contains frequencies of features by documents of your textual data. In other words, in this matrix rows represent documents and columns are features and the cells are the frequencies. 


```{r echo=TRUE}

#dfm
lang_tokens.3 <-tokens(lang_corp,remove_numbers = T,remove_punct = T) 
lang_tokens.3 <- tokens_compound(lang_tokens.3,pattern = phrase('programming language'))
lang_tokens.3 <- lang_tokens.3%>%tokens_remove(pattern = stopwords('en'))

lang_dfm <- dfm(lang_tokens.3)
docnames(lang_dfm) <- 1:5

#specs
ndoc(lang_dfm)
nfeat(lang_dfm)

#statistics
topfeatures(lang_dfm)

#weight
dfm_weight(lang_dfm,scheme = 'prop')

lang_dfm%>%dfm_tfidf()


#########select and remove
lang_dfm.2 <- dfm_remove(lang_dfm,pattern='Hello')

topfeatures(lang_dfm.2)


lang_dfm.3 <- dfm_select(lang_dfm,min_nchar = 4)

topfeatures(lang_dfm.3)


######### dfm trim

lang_dfm.4 <- dfm_trim(lang_dfm,max_docfreq = .2,docfreq_type = 'prop')



#### dfm group

lang_dfm.5 <- dfm_group(lang_dfm,groups = 'group')


## cooccurance matrix

fcm(lang_dfm)

```

### Statistical Analysis


```{r}
tf <- textstat_frequency(lang_dfm)

tf <- as.data.table(tf)
tf[,feature:=reorder(feature,frequency)]

ggplot(tf,aes(x=feature,y=frequency))+geom_point()+coord_flip()+labs(x=NULL,y='Word Frequency')+theme_bw()


textplot_wordcloud(lang_dfm)


```



```{r}


lang_dist <- as.dist(textstat_dist(lang_dfm))
clust <- hclust(lang_dist)
plot(clust, xlab = "Distance", ylab = NULL,labels = lang_data[,group])
lang_data

```
### Word Embedding

In short, word embdding is the name of a technique that the vocabulary are mapped to vectors where similar words represented close to each other. 



```{r}

lang_tokens.4 <-tokens(lang_data[,text],remove_numbers = T,remove_punct = T) 
lang_tokens.4 <- lang_tokens.4%>%tokens_remove(pattern = stopwords('en'))


lang_fcm <- fcm(lang_tokens.4, context = "window", count = "weighted", weights = 1 / (1:4), tri = TRUE,window = 4)


glove <- GlobalVectors$new(rank = 50,
                           x_max = 10)

lang_embed <- glove$fit_transform(lang_fcm, n_iter = 10,
                                  convergence_tol = 0.01, 
                                  n_threads = 8)


wv_context <- glove$components

word_vectors <- lang_embed + t(wv_context)

Python <- word_vectors["R", , drop = FALSE] -
  word_vectors["statistical", , drop = FALSE]+word_vectors["scripting", , drop = FALSE]

cos_sim <- textstat_simil(x = as.dfm(word_vectors), y = as.dfm(Python),
                          method = "cosine")

head(sort(cos_sim[, 1], decreasing = TRUE), n = 5)

```
### Supervised Learning: Text Classification

In this section we are going to use Naive Bayes supervised classifier. Before starting coding let me explain what does supervised mean. 

Supervised learning is a procedure that we are providing a pre labelled data set to train the classifier. We call this data set as training data set. Once we train classifier, we can predict categories of unseen data which is called as test data. You could check this [glossary](https://developers.google.com/machine-learning/glossary) for machine learning concepts.



```{r}
lang_dfm <- dfm(lang_tokens.3)
docnames(lang_dfm) <- 1:5

lang_training <- dfm_subset(lang_dfm,subset = docnames(lang_dfm) %in% c(1,2,5) )
lang_test <-dfm_subset(lang_dfm,subset = docnames(lang_dfm) %in% c(3,4) )

lang_model <- textmodel_nb(lang_training,lang_training$group)

summary(lang_model)

lang_test <- dfm_match(lang_test,featnames(lang_training))

actual_class <- lang_test$group
predicted_class <- predict(lang_model, newdata = lang_test)
tab_class <- table(actual_class, predicted_class)
tab_class


confusionMatrix(tab_class, mode = "everything")

```

### Unsupervised Learning: Topic Models

In this section we are not providing labels to the classifier. We only provide how many topics will be in the model. Then the algorithm distribute fetures to topics and topics to documents. I will use stm package to train topic models but you can try LDA as well. Since we are working with a dwarf data set the results are not pretty!

```{r}
lang_topic_model <- stm(documents = lang_dfm,K = 2,data = data.table(lang_dfm$group))

plot(lang_topic_model,n = 5)



td_beta <- tidy(lang_topic_model)

td_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  mutate(topic = paste0("Topic ", topic),
         term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free_y") +
  coord_flip() +
  scale_x_reordered() +
  labs(x = NULL, y = expression(beta),
       title = "Highest word probabilities for each topic",
       subtitle = "Different words are associated with different topics")




td_gamma <- tidy(lang_topic_model, matrix = "gamma",                    
                 document_names = rownames(lang_topic_model))

ggplot(td_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic) +
  labs(title = "Distribution of document probabilities for each topic",
       subtitle = "Each topic is associated with 1-3 stories",
       y = "Number of stories", x = expression(gamma))

```


